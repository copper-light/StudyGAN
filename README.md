### 1. Valina GAN
* Linear Layer 기반의 GAN
  * 랜덤 생성한 seed 값을 넣는데, 표준정규분포를 따르지 않고 완전 랜덤하게 넣으니까 학습이 되지 않음.
  * 레이어에 정규화 로직을 넣지 않으면 모드 붕괴가 일어남
  * 결국엔 위 두가지 모두 데이터가 특정 범위 내에 있다고 하더라도 데이터 정규화를 통해서 특정 기준을 중심으로 학습해야하는 것임

### 2. Conditional GAN
* Linear Layer 기반의 조건(생성할 클래스 지정) GAN
  * 생성기와 판별기 입력에 클래스 인코딩 값 넣으면 알아서 학습함

### 3. DCGAN (Deep Convolution GAN)
* CNN Based GAN
  * seed 값을 생성할때, 직접 2d 매트릭스에 해당하는 값을 생성했는데, linear하게 만들어내고 2d로 변환하는 것은 모델에게 학습하도록 하는것이 매우 결정적인 로직임
    * 모델에게 시드값의 임베딩을 직접 학습하도록 하는 것
  * 판별기와 생성기의 모델 수준은 유사해야함 -> 결국엔 판별기에서부터 흐르는 loss가 생성기의 품질을 결정하므로.
  * 생성기에서 전치Conv로 이미지 업샘플링할 때, 무조건 전치conv에 의지 하는 것보다 업샘플링은 필요할때 간결하게 하고 conv를 추가하여 이미지 품질을 높이는 방향으로 가는 것이 좋음 
  * 판별기에 dropout을 넣는 것과 안넣는 것은 하늘과 땅 차이임
    
* Conditional CNN GAN
  * 조건에 해당하는 label을 모델에게 넣을때 나름의 로직을 만들어서 2d onehot 으로 변환한 후에 생성 이미지와 합쳐서 2채널로 학습했는데, 이게 잘 안됐음
  * 이것도 원핫을 직접 2d 로 만들려고 하지 말고 이 또한 모델에게 원핫의 임베딩 값을 학습하도록 하는게 보다 나을 결과를 만들어냄

### 4. WGAN (Wasserstein GAN), WGAN-GP (Gradient Penalty)
* WGAN, WGAN-GP 모두 학습이 되도록 구현은 하였지만, DCGAN보다 더 나은지 모르겠다
  * 수렴되는게 눈에 보이긴하지만 품질이 더 나은지? 
  * WGAN 보다 GP가 더 안정적으로 수렴하는지 잘 모르겠음
  * 아마도 모델 크기가 작고 데이터도 mnist 로 했기 때문에 그런 것이라 생각됨
  * **따라서, 더 큰 모델, 더 복잡한 데이터를 생성하도록 테스트 해볼 필요 있음**
* Wasserstein GAN
  * 립시츠 제약을 거는 이유와 논리를 알아야함 
  * 웨이트 클립핑과 판별기 5회당 생성기 1회를 학습하기 때문에 학습의 속도가 느림
  * 클립핑 수치에 따라 모델 학습 원활 정도가 매우 상이함
  * 대신 loss 의 안정성은 있음
* WGAN-GP 
  * 가장 크게 오인 했던 것은, real - fake 로 로스를 0을 만드는 것을 목표로하는 것일줄 알았다는것
    * WGAN 과 WGAN-GP 로스의 목표는 0이 아니라 -를 향하도록 하는것임. 그래서 절대적인 기준이 아니라 잘하는 대로 흘러갈 것이기 때문에.
    * 실제는 fake - real 인데, fake 도 잘 학습하면 음수이고, real은 양수가 나오고 둘을 더하면 계속 음수로 흐르게됨
    * 만약 둘중에 하나라도 제대로 학습이 되지 않는다면 로스는 0에 가깝게 될 것임. 양수 - 양수, 음수 - 음수가 될 것이기에
  * 학습속도가 느린것은 동일
  * 다만, 더 안정성 있다고 하는데 비교해봐야할 듯
  * 잘 살펴볼 것은 Gradient Penalty Loss은 모델의 급격한 변화를 제한하기 위해서 1 사이의 노름을 먹여줌과 동시에 립시츠 제약을 검
    * 점점 0 에 수렴함에 따라 성능이 올라가는 것을 볼 수 있음
    * 모델의 학습 진척도를 짐작할 수 있는 수단으로 활용될 수 있을 듯